\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\Huge
\title{\Huge Artificial Intelligence II}
\usepackage{amsmath}
\author{\LARGE Nefeli Tavoulari}
\date{\LARGE Fall Semester 2021}
\usepackage{hyperref}
\begin{document}

\maketitle

\section{}\Large The Mean Squared Error loss function is defined as:\\ \\
 $\mathcal{MSE} $ =  $\frac{1}{m}$ $\sum_{i=1}^{m}(h_w(x_i) - y_i)^2$ \\ \\
 Calculating the gradient of MSE, we can find out the parameters which minimize the loss. \\ \\
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ =  $\nabla_{\!\mathbf{w}}\frac{1}{m}$ $\sum_{i=1}^{m}(h_w(x_i) - y_i)^2$ \\ \\
The features $x_i$ and the predicted values $y_i$ can be considered as constants, since the gradient is with respect to w, which is the modelâ€™s parameter vector. \\ (scalar multiplication rule) \\ \\
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ =  $\frac{1}{m}\nabla_{\!\mathbf{w}}$ $\sum_{i=1}^{m}(h_w(x_i) - y_i)^2$ \\ \\
Then, assuming we only have one instance (x,y) and using the power rule and the chain rule for derivatives: \\ \\
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $\nabla_{\!\mathbf{w}}$$(h_w(x) - y)^2$ \\ \\
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $2(h_w(x) - y)\nabla_{\!\mathbf{w}}$$(h_w(x) - y)$ \\ \\
where $h_w(x) = w_0x_0 + ... + w_nx_n = w^Tx: \textbf{(*)}$\\ \\
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $2(h_w(x) - y)\nabla (w_0x_0 + ... + w_nx_n - y)  $ \\ \\
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $2(h_w(x) - y)$$(\frac{\partial (w_0x_0 + ... + w_nx_n - y)}{\partial w_0} , ... , \frac{\partial (w_0x_0 + ... + w_nx_n - y)}{\partial w_n}) $ \\ \\ 
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $2(h_w(x) - y)$$(x_0 , ... , x_n)$ \\ \\ 
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $2(h_w(x) - y)$$(x)$ \\ \\ 
so, from \textbf{(*)}:\\ \\
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $2(w^Tx - y)$$(x)$   \\ \\ 
which equals to the following vector: \\ \\
$
\begin{bmatrix}
2(w^Tx - y)(x_0)\\
. . .\\             
2(w^Tx - y)(x_n)\\
\end{bmatrix}
$ \\ \\
Now, for m training instances: \\ \\ 
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $\frac{2}{m}\sum_{i=1}^{m}(h_w(x_i) - y_i)\nabla_{\!\mathbf{w}}$$(h_w(x_i) - y_i)$ \\ \\
$\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $\begin{bmatrix}
\frac{2}{m}(w^Tx_1 - y_1)(x_1,_0) + ... + \frac{2}{m}(w^Tx_m - y_m)(x_m,_0) \\
. . .\\             
\frac{2}{m}(w^Tx_1 - y_1)(x_1,_n) + ... + \frac{2}{m}(w^Tx_m - y_m)(x_m,_n)\\
\end{bmatrix}
$ \\ \\ 
So: $\nabla_{\!\mathbf{w}}\mathcal{MSE} $ = $\frac{2}{m}$(X$^T$(Xw - y)).
\section{Vaccine Sentiment Classification}
In this notebook I trained a multinomial Logistic Regression model, which classifies tweets as Neutral, Pro-vax or Anti-vax.
\subsection{Dataset}
The given dataset consists of two columns. The one column represents the tweets text and the other one the corresponding label (0, 1, 2 / neutral,anti-vax,pro-vax). Also, the dataset is rather imabalanced, since the anti-vax instances are almost 3 times fewer ,in comparison with the other classes. One conclusion we might reach out of it, is the fact that accuracy is probably not the right metric to evaluate our model.
\subsection{Preprocessing}
First of all, I performed some pre-processing and feature extraction on the data. I removed all empty or duplicate tweets, as they offer no useful information to the model, I lowercased tweets, performed lemmatization, removed stopwords, special characters, emojis, words written in a non-latin alphabet, urls and twitter accounts, in order for the model to focus on important and linguistically meaningful words, instead of noise. Also, I experimented with stemmers :
\begin{itemize}
    \item Porter (removes common endings of words)
    \item Lancaster (more aggressive)
    \item Snowball (better version of Porter)
\end{itemize} but I noticed that the Wordnet Lemmatizer performed much better than all of them, so I have commented out the lines that correspond to the stemmers in the notebook. In general, what I understood by my research is that lemmatization provides better results at times, because it performs a vocabulary and morphological analysis of the words and produces real, dictionary words, in contrast with stemming. \\ \\
The results I took from Stemming and Lemmatization, using CountVectorizer with TfidfTransformer and multinomial Logistic Regression on training and test data: \\ \\
\begin{Vmatrix}
& Porter & Lancaster & Snowball & Lemmatizer\\
Recall & 0.8214 & 0.8165 & 0.8210 & \textbf{0.8377}\\
Precision & 0.8214 & 0.8165 & 0.8210 &  \textbf{0.8377}\\
F1 & 0.8214 & 0.8165 & 0.8210 & \textbf{0.8377}\\
Accuracy & 0.8214 & 0.8165 & 0.8210 & \textbf{0.8377} \\
\end{Vmatrix}
\\ \\ \\
\begin{Vmatrix}
& Porter & Lancaster & Snowball & Lemmatizer\\
Recall & 0.7094 & 0.7085 & 0.7112 & \textbf{0.7177}\\
Precision & 0.7094 & 0.7085 & 0.7112 & \textbf{0.7177}\\
F1 & 0.7094 & 0.7085 & 0.7112 & \textbf{0.7177}\\
Accuracy & 0.7094 & 0.7085 & 0.7112 & \textbf{0.7177} \\
\end{Vmatrix}
\\ \\ \\
We observe here that the Snowball Stemmer really is a better version of the Porter Stemmer.
The Lemmatizer is the best solution though and so, from now on, this is the one I will be using.
\subsection{Feature Extraction}
Then, I vectorized and calculated TF-IDF on the textual data, so as to perform Logistic Regression. For this task, I tried out :
\begin{itemize}
\item CountVectorizer with TfidfTransformer, (computes word counts, IDF values and TF-IDF scores)
\item HashingVectorizer with TfidfTransformer, (uses hashing to find the token string name to feature integer index mapping)
\item TfidfVectorizer ( = CountVectorizer + TfidfTransformer)
\end{itemize}
The results on both training and testing data using multinomial Logistic Regression: \\ \\
\begin{Vmatrix}
& CountVec & HashingVec & TfidfVec\\
Recall & 0.8377 & 0.8287 & 0.8377\\
Precision & 0.8377 & 0.8287 & 0.8377\\
F1 &  0.8377 & 0.8287 & 0.8377\\
Accuracy & 0.8377 & 0.8287 & 0.8377\\
\end{Vmatrix} \\ \\ \\
\begin{Vmatrix}
& CountVec & HashingVec & TfidfVec\\
Recall & 0.7177 & 0.7169 & 0.7177\\
Precision & 0.7177 & 0.7169 & 0.7177\\
F1 & 0.7177 & 0.7169 & 0.7177\\
Accuracy & 0.7177 & 0.7169 & 0.7177\\
\end{Vmatrix} \\ \\ \\
We observe that the HashingVectorizer did not perform as well as the others. We also observe that the model might be overfitting, since the training score is quite higher than the test score. Thus, I performed some hyperparameter tuning, in order to improve the performance and avoid overfitting. \\ \\
For the \textbf{CountVectorizer}, I added max\_df and min\_df, in order to exclude some extremely rare or extremely used words, since they offer nothing but noise to the model. For the same reason, I also added the stopwords option (even though I had already done it manually in preprocessing). In this way, words like "the" or "asdfgh" are removed and that improves the performance of the model. Also, I added some regularization to the Logistic Regression model(C=1.5).\\ \\
For the \textbf{HashingVectorizer}, I added the stopwords option and enabled unigrams and bigrams also. Moreover, I added some regularization to the Logistic Regression model(C=1.9), elasticnet penalty, saga solver and l1\_ratio.\\ \\
For the \textbf{TfIdfVectorizer}, I added the same parameters as for the CountVectorizer.\\ \\
After hyperparameter tuning: \\ \\
\begin{Vmatrix}
& CountVec & HashingVec & TfidfVec\\
Recall & 0.7643 & \textbf{0.8535} & 0.7643\\
Precision & 0.7643 & \textbf{0.8535} & 0.7643\\
F1 & 0.7643 & \textbf{0.8535} & 0.7643\\
Accuracy & 0.7643 &	\textbf{0.8535} & 0.7643\\
\end{Vmatrix} \\ \\ \\
\begin{Vmatrix}
& CountVec & HashingVec & TfidfVec\\
Recall & 0.7230 & \textbf{0.7296} & 0.7230 \\
Precision & 0.7230 & \textbf{0.7296} & 0.7230\\
F1 & 0.7230 & \textbf{0.7296} &	0.7230\\
Accuracy & 0.7230 &	\textbf{0.7296} & 0.7230\\
\end{Vmatrix} \\ \\ \\
In general we observe that the training scores got worse than before hyperparameter tuning, but the test scores, which we are more interested in, were improved and that after all, the Hashing Vectorizer performs better than the others.

\subsection{More experiments}
I would also like to note that I experimented with RobustScaler and StandardScaler with Logistic Regression, but the results got worse in both cases, so I commented them out. \\ \\
Furthermore,I experimented with some other classifiers, to see whether the results would get any ameliorated.

\subsection{Evaluation}
In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance
Another thing we might observe is that "micro-F1 = micro-precision = micro-recall = accuracy" and 
Moreover, using different metrics, precision, recall, f1, accuracy, confusion matrix, I was able to see the quality of my model. From the confusion matrix, I understand that my model does not predict that well the anti-vaxers, but that is probably because the anti-vaxers in the data are very few, in comparison to the other classes.
\subsection{Plots}
I have added some plots where the most important/rare words are presented.
Also, I have a learning curve plot, where we can see that the model is not overfitting.
\subsection{In this assignment, I had the opportunity to familiarize with the concepts of data pre-processing, extraction, vectorization, visualization, but also with the process of making plenty of tests and research, in order to achieve the best result.}
\begin{thebibliography}{9}
\bibitem{}
\url{https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.YZPe0mBBwuV}
\end{thebibliography}

\end{document}

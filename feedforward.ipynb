{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vaccine.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW34__zUBaFL"
      },
      "source": [
        "# Vaccine Sentiment Classification\n",
        "*by Nefeli Tavoulari*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnk1un7nBfNR"
      },
      "source": [
        "#### In this notebook I classify tweets as Neutral, Pro-vax or Anti-vax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOXT2nsK1kOq"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl8wlD1VRC3F",
        "outputId": "603e7893-c6a8-4613-bfdb-347c183ffb78"
      },
      "source": [
        "!pip install -U torch==1.8.0 torchtext==0.9.0\n",
        "!pip install pyprind"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 14 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 17.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.62.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYnNIsloBipg"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB8j3615BTwM",
        "outputId": "a84eb5fd-5421-4dd7-858f-e6bb6165411a"
      },
      "source": [
        "%matplotlib inline\n",
        "import io\n",
        "import re\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from wordcloud import WordCloud\n",
        "import pyprind\n",
        "import nltk\n",
        "import re\n",
        "import csv\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.legacy import data   \n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.legacy.data import BucketIterator\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 557,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwQ1dAaDBrKg"
      },
      "source": [
        "## Upload dataset - Create dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mBmlpIlBt84",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f71bfdcd-7039-4661-8d33-56a8ef13fe01"
      },
      "source": [
        "upload_train = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bb337244-7d13-464d-baf9-406c37092f24\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bb337244-7d13-464d-baf9-406c37092f24\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving vs_train.csv to vs_train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqzdiqaTBv_E",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "78fd1b1c-5de9-4ef4-8a0a-8a01d22a73d4"
      },
      "source": [
        "upload_dev = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b3acfdf5-6077-40ed-8943-250b538978e6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b3acfdf5-6077-40ed-8943-250b538978e6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving vs_dev.csv to vs_dev.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crl8Sc9YByK9"
      },
      "source": [
        "train_df = pd.read_csv(io.BytesIO(upload_train['vs_train.csv']))\n",
        "dev_df = pd.read_csv(io.BytesIO(upload_dev['vs_dev.csv']))"
      ],
      "execution_count": 569,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKStHo66Bz8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4639ee-c034-4575-b25a-31d4773da2d3"
      },
      "source": [
        "print(train_df) # training data"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Unnamed: 0                                              tweet  label\n",
            "0               0  Sip N Shop Come thru right now #Marjais #Popul...      0\n",
            "1               1  I don't know about you but My family and I wil...      1\n",
            "2               2  @MSignorile Immunizations should be mandatory....      2\n",
            "3               3  President Obama spoke in favor of vaccination ...      0\n",
            "4               4  \"@myfoxla: Arizona monitoring hundreds for mea...      0\n",
            "...           ...                                                ...    ...\n",
            "15971       15971  @Salon if u believe the anti-vax nutcases caus...      1\n",
            "15972       15972  How do you feel about parents who don't #vacci...      0\n",
            "15973       15973  70 Preschoolers Tested for Measles in Simi Val...      0\n",
            "15974       15974  Finance Minister: Budget offers room to procur...      0\n",
            "15975       15975  Are you up to date on vaccines? Take CDC’s vac...      2\n",
            "\n",
            "[15976 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W3hygNmB05X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6f20ae-7fa6-4f9e-d1ae-84e7c4253217"
      },
      "source": [
        "print(dev_df) # validation data"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Unnamed: 0                                              tweet  label\n",
            "0              0  @user They had a massive surge in with covid d...      1\n",
            "1              1  Required vaccines for school: Parents and guar...      0\n",
            "2              2  “@KCStar: Two more Johnson County children hav...      0\n",
            "3              3  NV can do better. Which states are the best (a...      2\n",
            "4              4  Nothing like killing ourselves w/ our own fear...      2\n",
            "...          ...                                                ...    ...\n",
            "2277        2277  RT @abc7: Number of measles cases reported in ...      0\n",
            "2278        2278  Evidence points to the idea that \"measles affe...      0\n",
            "2279        2279  Where's @SavedYouAClick \"@voxdotcom: Why you s...      2\n",
            "2280        2280  Some of my favorite people have autism. If tha...      2\n",
            "2281        2281  Coronavirus: The married couple behind the suc...      0\n",
            "\n",
            "[2282 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrknN4jRbeN6"
      },
      "source": [
        "## Upload Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8v30jvy8JOx"
      },
      "source": [
        "def load_glove_model(File):\n",
        "    print(\"Loading Glove Model\")\n",
        "    glove_model = {}\n",
        "    with open(File,'r') as f:\n",
        "        for line in f:\n",
        "            split_line = line.split()\n",
        "            word = split_line[0]\n",
        "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "            glove_model[word] = embedding\n",
        "    print(f\"{len(glove_model)} words loaded!\")\n",
        "    return glove_model"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xukjiAFj86dI"
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove.twitter.27B.zip\n",
        "# free some space\n",
        "!rm glove.twitter.27B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLj-OVw0mFuv",
        "outputId": "9ebe421c-f926-4633-9312-3c0a63f31d92"
      },
      "source": [
        "gloveModel = load_glove_model(\"glove.twitter.27B.25d.txt\")\n",
        "dim = 25"
      ],
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Glove Model\n",
            "1193514 words loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztXIRmkHGXVN"
      },
      "source": [
        "## Remove empty / duplicate tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_A522pkGZLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040f6181-0a45-4498-f92b-937d91d931c6"
      },
      "source": [
        "train_df.dropna(subset = [\"tweet\"], inplace=True)\n",
        "train_df.drop_duplicates(subset = [\"tweet\"], inplace=True)\n",
        "\n",
        "dev_df.dropna(subset = [\"tweet\"], inplace=True)\n",
        "\n",
        "train_df.drop(['Unnamed: 0'], axis=1, inplace = True) \n",
        "dev_df.drop(['Unnamed: 0'], axis=1, inplace = True) \n",
        "\n",
        "print(train_df.shape)\n",
        "print(dev_df.shape)"
      ],
      "execution_count": 570,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15881, 2)\n",
            "(2282, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK5Qud3-cAYm"
      },
      "source": [
        "## Clean text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gco1RADEpk1w"
      },
      "source": [
        "def clean_text(text):\n",
        "  text = text.lower()                                           # lowercase\n",
        "  text = text.strip()                                           # remove white spaces\n",
        "  #text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)\",\" \",text).split())   # remove twitter user accounts\n",
        "  #text = re.sub(r'http\\S+', ' ', text)                          # remove urls\n",
        "  #text = re.sub('[^A-Za-z0-9]+', ' ', text)                     # remove special characters\n",
        "  # perform lemmatization\n",
        "  cleaned_text = \"\"\n",
        "  for word in text.split() :\n",
        "    #if word in stop_words:                                      # remove stopwords\n",
        "    #  continue\n",
        "    temp = lemmatizer.lemmatize(word)                           # lemmatize\n",
        "    #temp = snowball.stem(word)\n",
        "    #temp = lancaster.stem(word)\n",
        "    #temp = porter.stem(word)\n",
        "    cleaned_text += (temp + \" \")\n",
        "  return cleaned_text"
      ],
      "execution_count": 571,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvL7F6BKcT5c"
      },
      "source": [
        "cleaned_text = []                                  # clean training data\n",
        "for line in train_df[\"tweet\"]:\n",
        "  cleaned_text.append(clean_text(line))\n",
        "cleaned_text_val = []                              # clean validation data\n",
        "for line in dev_df[\"tweet\"]:\n",
        "  cleaned_text_val.append(clean_text(line))\n",
        "\n",
        "train_df = train_df.assign(clean_tweet = lambda x: cleaned_text)\n",
        "dev_df = dev_df.assign(clean_tweet = lambda x: cleaned_text_val)\n",
        "\n",
        "train_df.drop(['tweet'], axis=1, inplace = True) \n",
        "dev_df.drop(['tweet'], axis=1, inplace = True) "
      ],
      "execution_count": 572,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4up9Sn_dYHIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4664ed62-769a-453d-b6f0-fad731f1497e"
      },
      "source": [
        "total_df = train_df.append(dev_df)\n",
        "total_df.shape"
      ],
      "execution_count": 573,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18163, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 573
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs10h234cr4V"
      },
      "source": [
        "## Use Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQrnaHErAf9I"
      },
      "source": [
        "vocab = {}\n",
        "for row in range(0, total_df.shape[0]):\n",
        "  vocab[row] = []\n",
        "i = 0\n",
        "for tweet in total_df[\"clean_tweet\"]: # one tweet\n",
        "  count = np.zeros(dim)  \n",
        "  for word in tweet.split():\n",
        "    if (word in gloveModel):\n",
        "      count += gloveModel[word]\n",
        "  for num in count/dim:\n",
        "    vocab[i].append(num)\n",
        "  i += 1"
      ],
      "execution_count": 574,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "sD9rqxzcFCZj",
        "outputId": "422752ef-6429-4ac8-a5eb-5c7fe768ba54"
      },
      "source": [
        "df = pd.DataFrame.from_dict(vocab, orient='index')\n",
        "#df = df.transpose()\n",
        "df"
      ],
      "execution_count": 575,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.126980</td>\n",
              "      <td>0.145228</td>\n",
              "      <td>0.045882</td>\n",
              "      <td>-0.070324</td>\n",
              "      <td>-0.061124</td>\n",
              "      <td>-0.017800</td>\n",
              "      <td>0.296688</td>\n",
              "      <td>0.121141</td>\n",
              "      <td>-0.025490</td>\n",
              "      <td>0.145013</td>\n",
              "      <td>-0.097822</td>\n",
              "      <td>-0.040530</td>\n",
              "      <td>-1.221152</td>\n",
              "      <td>-0.057454</td>\n",
              "      <td>-0.033446</td>\n",
              "      <td>-0.109443</td>\n",
              "      <td>0.010085</td>\n",
              "      <td>-0.180925</td>\n",
              "      <td>-0.191412</td>\n",
              "      <td>-0.042126</td>\n",
              "      <td>-0.052271</td>\n",
              "      <td>-0.150326</td>\n",
              "      <td>0.144013</td>\n",
              "      <td>0.133561</td>\n",
              "      <td>0.141724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.127791</td>\n",
              "      <td>0.281733</td>\n",
              "      <td>-0.003274</td>\n",
              "      <td>-0.053801</td>\n",
              "      <td>-0.257891</td>\n",
              "      <td>-0.137699</td>\n",
              "      <td>0.884498</td>\n",
              "      <td>-0.161585</td>\n",
              "      <td>-0.143887</td>\n",
              "      <td>0.072430</td>\n",
              "      <td>-0.209006</td>\n",
              "      <td>0.238301</td>\n",
              "      <td>-3.120783</td>\n",
              "      <td>-0.079732</td>\n",
              "      <td>-0.011743</td>\n",
              "      <td>0.040789</td>\n",
              "      <td>0.199981</td>\n",
              "      <td>-0.326513</td>\n",
              "      <td>-0.042994</td>\n",
              "      <td>-0.235782</td>\n",
              "      <td>0.056373</td>\n",
              "      <td>0.175068</td>\n",
              "      <td>-0.036683</td>\n",
              "      <td>0.089611</td>\n",
              "      <td>-0.159751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.006619</td>\n",
              "      <td>0.288750</td>\n",
              "      <td>-0.007621</td>\n",
              "      <td>-0.137981</td>\n",
              "      <td>-0.188718</td>\n",
              "      <td>-0.062341</td>\n",
              "      <td>0.511346</td>\n",
              "      <td>-0.401100</td>\n",
              "      <td>-0.029118</td>\n",
              "      <td>0.046303</td>\n",
              "      <td>-0.135412</td>\n",
              "      <td>0.137954</td>\n",
              "      <td>-2.235289</td>\n",
              "      <td>-0.043798</td>\n",
              "      <td>0.048197</td>\n",
              "      <td>0.064200</td>\n",
              "      <td>0.103578</td>\n",
              "      <td>-0.160487</td>\n",
              "      <td>0.027403</td>\n",
              "      <td>-0.264605</td>\n",
              "      <td>-0.069611</td>\n",
              "      <td>0.228390</td>\n",
              "      <td>0.012685</td>\n",
              "      <td>-0.089903</td>\n",
              "      <td>-0.033591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.089060</td>\n",
              "      <td>0.326082</td>\n",
              "      <td>-0.116685</td>\n",
              "      <td>-0.144412</td>\n",
              "      <td>-0.092584</td>\n",
              "      <td>-0.405214</td>\n",
              "      <td>0.403942</td>\n",
              "      <td>-0.332710</td>\n",
              "      <td>0.062130</td>\n",
              "      <td>-0.093784</td>\n",
              "      <td>0.110344</td>\n",
              "      <td>0.141606</td>\n",
              "      <td>-2.956036</td>\n",
              "      <td>0.128259</td>\n",
              "      <td>0.230753</td>\n",
              "      <td>-0.105243</td>\n",
              "      <td>-0.090958</td>\n",
              "      <td>0.018877</td>\n",
              "      <td>-0.085146</td>\n",
              "      <td>-0.413468</td>\n",
              "      <td>-0.113656</td>\n",
              "      <td>0.159633</td>\n",
              "      <td>-0.050204</td>\n",
              "      <td>-0.375640</td>\n",
              "      <td>-0.159524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.034376</td>\n",
              "      <td>0.208181</td>\n",
              "      <td>-0.031563</td>\n",
              "      <td>-0.023554</td>\n",
              "      <td>-0.162232</td>\n",
              "      <td>-0.111582</td>\n",
              "      <td>0.292521</td>\n",
              "      <td>-0.408276</td>\n",
              "      <td>0.001760</td>\n",
              "      <td>-0.055854</td>\n",
              "      <td>0.081972</td>\n",
              "      <td>-0.055733</td>\n",
              "      <td>-1.703889</td>\n",
              "      <td>0.172195</td>\n",
              "      <td>0.149702</td>\n",
              "      <td>-0.178134</td>\n",
              "      <td>0.073103</td>\n",
              "      <td>-0.042976</td>\n",
              "      <td>0.120007</td>\n",
              "      <td>-0.221171</td>\n",
              "      <td>-0.249586</td>\n",
              "      <td>-0.030080</td>\n",
              "      <td>-0.019676</td>\n",
              "      <td>-0.305006</td>\n",
              "      <td>-0.091714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18158</th>\n",
              "      <td>0.015521</td>\n",
              "      <td>0.217046</td>\n",
              "      <td>0.133655</td>\n",
              "      <td>-0.122387</td>\n",
              "      <td>-0.094617</td>\n",
              "      <td>-0.119492</td>\n",
              "      <td>0.658689</td>\n",
              "      <td>0.077358</td>\n",
              "      <td>-0.041994</td>\n",
              "      <td>0.074271</td>\n",
              "      <td>-0.044117</td>\n",
              "      <td>0.048257</td>\n",
              "      <td>-3.200944</td>\n",
              "      <td>0.159769</td>\n",
              "      <td>0.234162</td>\n",
              "      <td>-0.112080</td>\n",
              "      <td>0.206225</td>\n",
              "      <td>-0.071724</td>\n",
              "      <td>-0.201507</td>\n",
              "      <td>-0.196546</td>\n",
              "      <td>-0.046429</td>\n",
              "      <td>0.072961</td>\n",
              "      <td>0.101380</td>\n",
              "      <td>-0.151878</td>\n",
              "      <td>-0.013574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18159</th>\n",
              "      <td>0.140394</td>\n",
              "      <td>0.114400</td>\n",
              "      <td>-0.061038</td>\n",
              "      <td>0.044528</td>\n",
              "      <td>-0.004276</td>\n",
              "      <td>-0.068286</td>\n",
              "      <td>0.781928</td>\n",
              "      <td>-0.365074</td>\n",
              "      <td>0.118210</td>\n",
              "      <td>-0.039317</td>\n",
              "      <td>-0.016753</td>\n",
              "      <td>0.088055</td>\n",
              "      <td>-3.058932</td>\n",
              "      <td>0.260586</td>\n",
              "      <td>0.201300</td>\n",
              "      <td>-0.074426</td>\n",
              "      <td>0.167593</td>\n",
              "      <td>-0.039346</td>\n",
              "      <td>0.113259</td>\n",
              "      <td>-0.336512</td>\n",
              "      <td>-0.256375</td>\n",
              "      <td>0.047348</td>\n",
              "      <td>0.080381</td>\n",
              "      <td>-0.125853</td>\n",
              "      <td>-0.318716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18160</th>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.122073</td>\n",
              "      <td>-0.011033</td>\n",
              "      <td>-0.021156</td>\n",
              "      <td>-0.105546</td>\n",
              "      <td>-0.129614</td>\n",
              "      <td>0.341136</td>\n",
              "      <td>-0.201408</td>\n",
              "      <td>-0.124126</td>\n",
              "      <td>0.079042</td>\n",
              "      <td>-0.033622</td>\n",
              "      <td>0.076532</td>\n",
              "      <td>-1.367574</td>\n",
              "      <td>-0.052747</td>\n",
              "      <td>-0.048435</td>\n",
              "      <td>-0.025753</td>\n",
              "      <td>0.076361</td>\n",
              "      <td>-0.146981</td>\n",
              "      <td>0.022983</td>\n",
              "      <td>-0.128196</td>\n",
              "      <td>-0.050226</td>\n",
              "      <td>0.088467</td>\n",
              "      <td>-0.004106</td>\n",
              "      <td>-0.055301</td>\n",
              "      <td>-0.144634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18161</th>\n",
              "      <td>-0.047579</td>\n",
              "      <td>0.424718</td>\n",
              "      <td>-0.061285</td>\n",
              "      <td>0.149231</td>\n",
              "      <td>-0.146165</td>\n",
              "      <td>-0.108311</td>\n",
              "      <td>0.928633</td>\n",
              "      <td>-0.277970</td>\n",
              "      <td>-0.343805</td>\n",
              "      <td>0.069508</td>\n",
              "      <td>-0.185781</td>\n",
              "      <td>0.213418</td>\n",
              "      <td>-3.334744</td>\n",
              "      <td>-0.127521</td>\n",
              "      <td>-0.013192</td>\n",
              "      <td>0.006213</td>\n",
              "      <td>0.235753</td>\n",
              "      <td>-0.324147</td>\n",
              "      <td>0.124122</td>\n",
              "      <td>-0.154793</td>\n",
              "      <td>-0.081905</td>\n",
              "      <td>0.078724</td>\n",
              "      <td>-0.007525</td>\n",
              "      <td>0.010189</td>\n",
              "      <td>-0.248181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18162</th>\n",
              "      <td>-0.181870</td>\n",
              "      <td>0.283055</td>\n",
              "      <td>-0.059761</td>\n",
              "      <td>-0.087506</td>\n",
              "      <td>0.045846</td>\n",
              "      <td>-0.208841</td>\n",
              "      <td>0.565342</td>\n",
              "      <td>-0.258200</td>\n",
              "      <td>0.089651</td>\n",
              "      <td>-0.102176</td>\n",
              "      <td>0.133229</td>\n",
              "      <td>0.101730</td>\n",
              "      <td>-2.668339</td>\n",
              "      <td>0.114279</td>\n",
              "      <td>-0.020560</td>\n",
              "      <td>-0.099332</td>\n",
              "      <td>0.115584</td>\n",
              "      <td>-0.164970</td>\n",
              "      <td>-0.023118</td>\n",
              "      <td>-0.161979</td>\n",
              "      <td>-0.135321</td>\n",
              "      <td>0.124701</td>\n",
              "      <td>-0.281872</td>\n",
              "      <td>-0.512609</td>\n",
              "      <td>-0.142532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18163 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2   ...        22        23        24\n",
              "0     -0.126980  0.145228  0.045882  ...  0.144013  0.133561  0.141724\n",
              "1     -0.127791  0.281733 -0.003274  ... -0.036683  0.089611 -0.159751\n",
              "2     -0.006619  0.288750 -0.007621  ...  0.012685 -0.089903 -0.033591\n",
              "3      0.089060  0.326082 -0.116685  ... -0.050204 -0.375640 -0.159524\n",
              "4     -0.034376  0.208181 -0.031563  ... -0.019676 -0.305006 -0.091714\n",
              "...         ...       ...       ...  ...       ...       ...       ...\n",
              "18158  0.015521  0.217046  0.133655  ...  0.101380 -0.151878 -0.013574\n",
              "18159  0.140394  0.114400 -0.061038  ...  0.080381 -0.125853 -0.318716\n",
              "18160  0.010588  0.122073 -0.011033  ... -0.004106 -0.055301 -0.144634\n",
              "18161 -0.047579  0.424718 -0.061285  ... -0.007525  0.010189 -0.248181\n",
              "18162 -0.181870  0.283055 -0.059761  ... -0.281872 -0.512609 -0.142532\n",
              "\n",
              "[18163 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 575
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKSYQD9YMcHd"
      },
      "source": [
        "train = df.iloc[:train_df.shape[0], :]\n",
        "dev = df.iloc[train_df.shape[0]:, :]\n",
        "\n",
        "x = torch.tensor(df.values, dtype=torch.float)\n",
        "y = torch.tensor(total_df['label'].values, dtype=torch.float)\n",
        "\n",
        "x_train = torch.tensor(train.values, dtype=torch.float)\n",
        "y_train = torch.tensor(train_df['label'].values, dtype=torch.float)\n",
        "\n",
        "x_dev = torch.tensor(dev.values, dtype=torch.float)\n",
        "y_dev = torch.tensor(dev_df['label'].values, dtype=torch.float)\n",
        "\n",
        "# Initialize dataloaders\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "dev_dataset = TensorDataset(x_dev, y_dev)\n",
        "dev_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 576,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkBnnj16ZTC5"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.linear1 = nn.Linear(D_in, H1)\n",
        "        self.linear2 = nn.Linear(H1, H2)\n",
        "        self.linear3 = nn.Linear(H2, H3)\n",
        "        self.linear4 = nn.Linear(H3, D_out)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        #nn.LogSoftmax\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h1 = self.linear1(x)\n",
        "        h2 = self.linear2(h1)\n",
        "        h3 = self.linear3(h2)\n",
        "        #h3 = self.relu(h3)\n",
        "        out = self.linear4(h3)\n",
        "        #out = self.softmax(out)\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 577,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW7kkrK-ZZ3e"
      },
      "source": [
        "#Define layer sizes\n",
        "D_in = x.shape[1]\n",
        "H1 = 128\n",
        "H2 = 64\n",
        "H3 = 32\n",
        "D_out = 1\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 1e-4\n",
        "\n",
        "#Initialize model, loss, optimizer\n",
        "model = Net(D_in, H1, H2, H3, D_out)\n",
        "loss_func = nn.MSELoss(reduction='sum')\n",
        "#nn.BCELoss\n",
        "#loss_func = nn.CrossEntropyLoss()#weight=class_weights)\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=learning_rate)#, momentum=0.2)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 586,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvAyCpStZ3j1",
        "outputId": "e578de9f-4801-4634-cf6b-b39a12500e59"
      },
      "source": [
        "model"
      ],
      "execution_count": 579,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (linear1): Linear(in_features=25, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (linear4): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (softmax): Softmax(dim=0)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 579
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtP2ZPiMZ5-A",
        "outputId": "643f8a67-8157-476c-bee4-e38b27802389"
      },
      "source": [
        "for epoch in range(100):\n",
        "  batch_losses = []\n",
        "\n",
        "  # training\n",
        "  for x_batch, y_batch in train_dataloader:  # for each tweet\n",
        "    y_pred = model(x_batch).squeeze(1)\n",
        "    loss = loss_func(y_pred, y_batch)\n",
        "    batch_losses.append(loss.item())\n",
        "    \n",
        "    #Delete previously stored gradients\n",
        "    optimizer.zero_grad()\n",
        "    #Perform backpropagation starting from the loss calculated in this epoch\n",
        "    loss.backward()\n",
        "    #Update model's weights based on the gradients calculated during backprop\n",
        "    optimizer.step()\n",
        "\n",
        "  # validation    \n",
        "  with torch.no_grad():\n",
        "    batch_losses_dev = []\n",
        "    model.eval()\n",
        "\n",
        "    for x_batch, y_batch in dev_dataloader:\n",
        "        y_dev_pred = model(x_batch).squeeze(1)\n",
        "        loss_dev = loss_func(y_dev_pred, y_batch)\n",
        "        batch_losses_dev.append(loss_dev.item())\n",
        "  \n",
        "  print(f\"Epoch {epoch:3}: Train Loss = {sum(batch_losses)/len(dataloader):.5f}   Validation Loss = {sum(batch_losses_dev)/len(dev_dataloader):.5f}\")"
      ],
      "execution_count": 585,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0: Train Loss = 59.18598   Validation Loss = 55.28262\n",
            "Epoch   1: Train Loss = 54.73597   Validation Loss = 54.48002\n",
            "Epoch   2: Train Loss = 54.41821   Validation Loss = 54.37278\n",
            "Epoch   3: Train Loss = 54.34967   Validation Loss = 54.32664\n",
            "Epoch   4: Train Loss = 54.30806   Validation Loss = 54.28760\n",
            "Epoch   5: Train Loss = 54.27137   Validation Loss = 54.25214\n",
            "Epoch   6: Train Loss = 54.23660   Validation Loss = 54.21736\n",
            "Epoch   7: Train Loss = 54.20126   Validation Loss = 54.18292\n",
            "Epoch   8: Train Loss = 54.16705   Validation Loss = 54.14834\n",
            "Epoch   9: Train Loss = 54.13277   Validation Loss = 54.11391\n",
            "Epoch  10: Train Loss = 54.09833   Validation Loss = 54.07948\n",
            "Epoch  11: Train Loss = 54.06406   Validation Loss = 54.04516\n",
            "Epoch  12: Train Loss = 54.02982   Validation Loss = 54.01068\n",
            "Epoch  13: Train Loss = 53.99555   Validation Loss = 53.97629\n",
            "Epoch  14: Train Loss = 53.96028   Validation Loss = 53.94192\n",
            "Epoch  15: Train Loss = 53.92621   Validation Loss = 53.90721\n",
            "Epoch  16: Train Loss = 53.89193   Validation Loss = 53.87245\n",
            "Epoch  17: Train Loss = 53.85692   Validation Loss = 53.83755\n",
            "Epoch  18: Train Loss = 53.82135   Validation Loss = 53.80235\n",
            "Epoch  19: Train Loss = 53.78667   Validation Loss = 53.76665\n",
            "Epoch  20: Train Loss = 53.75128   Validation Loss = 53.73048\n",
            "Epoch  21: Train Loss = 53.71423   Validation Loss = 53.69366\n",
            "Epoch  22: Train Loss = 53.67716   Validation Loss = 53.65603\n",
            "Epoch  23: Train Loss = 53.63919   Validation Loss = 53.61754\n",
            "Epoch  24: Train Loss = 53.59844   Validation Loss = 53.57775\n",
            "Epoch  25: Train Loss = 53.55969   Validation Loss = 53.53617\n",
            "Epoch  26: Train Loss = 53.51716   Validation Loss = 53.49272\n",
            "Epoch  27: Train Loss = 53.47312   Validation Loss = 53.44706\n",
            "Epoch  28: Train Loss = 53.42581   Validation Loss = 53.39887\n",
            "Epoch  29: Train Loss = 53.37613   Validation Loss = 53.34692\n",
            "Epoch  30: Train Loss = 53.32287   Validation Loss = 53.29113\n",
            "Epoch  31: Train Loss = 53.26417   Validation Loss = 53.23108\n",
            "Epoch  32: Train Loss = 53.20196   Validation Loss = 53.16410\n",
            "Epoch  33: Train Loss = 53.13192   Validation Loss = 53.09085\n",
            "Epoch  34: Train Loss = 53.05583   Validation Loss = 53.00932\n",
            "Epoch  35: Train Loss = 52.97014   Validation Loss = 52.91837\n",
            "Epoch  36: Train Loss = 52.87409   Validation Loss = 52.81614\n",
            "Epoch  37: Train Loss = 52.76485   Validation Loss = 52.70122\n",
            "Epoch  38: Train Loss = 52.64286   Validation Loss = 52.57446\n",
            "Epoch  39: Train Loss = 52.51038   Validation Loss = 52.42971\n",
            "Epoch  40: Train Loss = 52.35959   Validation Loss = 52.27404\n",
            "Epoch  41: Train Loss = 52.20196   Validation Loss = 52.11251\n",
            "Epoch  42: Train Loss = 52.04091   Validation Loss = 51.95313\n",
            "Epoch  43: Train Loss = 51.87092   Validation Loss = 51.82765\n",
            "Epoch  44: Train Loss = 51.72445   Validation Loss = 51.63510\n",
            "Epoch  45: Train Loss = 51.57113   Validation Loss = 51.49955\n",
            "Epoch  46: Train Loss = 51.45034   Validation Loss = 51.39732\n",
            "Epoch  47: Train Loss = 51.32993   Validation Loss = 51.29643\n",
            "Epoch  48: Train Loss = 51.22540   Validation Loss = 51.17985\n",
            "Epoch  49: Train Loss = 51.14754   Validation Loss = 51.08432\n",
            "Epoch  50: Train Loss = 51.05864   Validation Loss = 51.04894\n",
            "Epoch  51: Train Loss = 50.99172   Validation Loss = 50.93310\n",
            "Epoch  52: Train Loss = 50.91639   Validation Loss = 50.91160\n",
            "Epoch  53: Train Loss = 50.85777   Validation Loss = 50.81391\n",
            "Epoch  54: Train Loss = 50.79836   Validation Loss = 50.74389\n",
            "Epoch  55: Train Loss = 50.74155   Validation Loss = 50.68868\n",
            "Epoch  56: Train Loss = 50.67033   Validation Loss = 50.84479\n",
            "Epoch  57: Train Loss = 50.65297   Validation Loss = 50.58956\n",
            "Epoch  58: Train Loss = 50.59307   Validation Loss = 50.53928\n",
            "Epoch  59: Train Loss = 50.53725   Validation Loss = 50.50860\n",
            "Epoch  60: Train Loss = 50.50031   Validation Loss = 50.46884\n",
            "Epoch  61: Train Loss = 50.44817   Validation Loss = 50.51967\n",
            "Epoch  62: Train Loss = 50.42212   Validation Loss = 50.38808\n",
            "Epoch  63: Train Loss = 50.38664   Validation Loss = 50.33649\n",
            "Epoch  64: Train Loss = 50.34764   Validation Loss = 50.30132\n",
            "Epoch  65: Train Loss = 50.30609   Validation Loss = 50.27244\n",
            "Epoch  66: Train Loss = 50.27851   Validation Loss = 50.28157\n",
            "Epoch  67: Train Loss = 50.26032   Validation Loss = 50.20743\n",
            "Epoch  68: Train Loss = 50.22321   Validation Loss = 50.22738\n",
            "Epoch  69: Train Loss = 50.19306   Validation Loss = 50.15082\n",
            "Epoch  70: Train Loss = 50.17791   Validation Loss = 50.12630\n",
            "Epoch  71: Train Loss = 50.14841   Validation Loss = 50.10421\n",
            "Epoch  72: Train Loss = 50.13788   Validation Loss = 50.08492\n",
            "Epoch  73: Train Loss = 50.10003   Validation Loss = 50.07079\n",
            "Epoch  74: Train Loss = 50.07861   Validation Loss = 50.04443\n",
            "Epoch  75: Train Loss = 50.05881   Validation Loss = 50.02582\n",
            "Epoch  76: Train Loss = 50.03999   Validation Loss = 50.02161\n",
            "Epoch  77: Train Loss = 50.01571   Validation Loss = 49.99323\n",
            "Epoch  78: Train Loss = 50.01167   Validation Loss = 49.97122\n",
            "Epoch  79: Train Loss = 49.99943   Validation Loss = 49.95983\n",
            "Epoch  80: Train Loss = 49.98587   Validation Loss = 49.94320\n",
            "Epoch  81: Train Loss = 49.96623   Validation Loss = 49.93659\n",
            "Epoch  82: Train Loss = 49.95811   Validation Loss = 49.94833\n",
            "Epoch  83: Train Loss = 49.96283   Validation Loss = 49.92605\n",
            "Epoch  84: Train Loss = 49.94317   Validation Loss = 49.91158\n",
            "Epoch  85: Train Loss = 49.91080   Validation Loss = 49.99395\n",
            "Epoch  86: Train Loss = 49.92464   Validation Loss = 49.90221\n",
            "Epoch  87: Train Loss = 49.91298   Validation Loss = 49.99777\n",
            "Epoch  88: Train Loss = 49.87996   Validation Loss = 49.85544\n",
            "Epoch  89: Train Loss = 49.89990   Validation Loss = 49.89099\n",
            "Epoch  90: Train Loss = 49.88748   Validation Loss = 49.84337\n",
            "Epoch  91: Train Loss = 49.88327   Validation Loss = 49.82434\n",
            "Epoch  92: Train Loss = 49.84395   Validation Loss = 50.02496\n",
            "Epoch  93: Train Loss = 49.84316   Validation Loss = 49.92404\n",
            "Epoch  94: Train Loss = 49.85514   Validation Loss = 49.80251\n",
            "Epoch  95: Train Loss = 49.84168   Validation Loss = 49.86807\n",
            "Epoch  96: Train Loss = 49.83523   Validation Loss = 49.78758\n",
            "Epoch  97: Train Loss = 49.83925   Validation Loss = 49.78210\n",
            "Epoch  98: Train Loss = 49.83303   Validation Loss = 49.77901\n",
            "Epoch  99: Train Loss = 49.83219   Validation Loss = 49.76785\n"
          ]
        }
      ]
    }
  ]
}